---
title: "Selección de Modelos"
output:
  html_document:
    css: tutorial.css
    fig_caption: yes
    highlight: pygments
   # theme: simplex
    toc: yes
    toc_float: yes
---

```{=html}
<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #437d66;
}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r, eval=TRUE, echo=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}

#`r colorize("some words in red", "red")`
library(ggplot2)
rlt_theme <- theme(axis.title.y = element_text(colour="grey20",size=15,face="bold"),
        axis.text.x = element_text(colour="grey20",size=15, face="bold"),
        axis.text.y = element_text(colour="grey20",size=15,face="bold"),  
        axis.title.x = element_text(colour="grey20",size=15,face="bold"))+
        theme_bw()
```

```{r, echo=FALSE, fig.show = "hold", out.width = "50%", fig.align = "default"}
knitr::include_graphics(c("Graficos/Analitica.png", "Graficos/UPR_IPERT_logo.png"))
```

------------------------------------------------------------------------

# Datos, hipótesis y modelos

La ciencia es un proceso para aprender sobre nuestro entorno en donde múltiples ideas, de vez en cuando contradictorias, trata de explicar como el mundo funciona. Típicamente uno comienza con una idea, una expresión verbal, que se desarrolla como una hipótesis. Luego es necesario expresar esa hipótesis con una ecuación matemática o sea un modelo. El concepto de modelos en la ciencia es que un modelo explica un proceso simplificado (por ejemplo biológico) que da una apreciación sobre los factores que son responsables por el patrón observado. Por consecuencia cuan bien los datos reflejan el modelo también refleja un apoyo sobre la hipótesis.

Hay dos acercamientos principales para inferir. El método tradicional es determinar si la hipótesis nula puede ser rechazada basado en un conjunto de datos. La hipótesis es rechazada cuando una prueba estadística resulta en un valor por encima de un umbral, típicamente p \< 0.05. Rechazar la hipótesis nula es evidencia para muchos que la hipótesis alterna es CORRECTA. Aunque parece ser un juego de palabra, es que realmente estamos solamente diciendo que `r colorize("no hay apoyo para la hipótesis nula", "red")`. La razón es que no estamos probando si la hipótesis alterna es `r colorize("veridica", "red")`. Es importante recordar que la descripción de la hipótesis alterna (la descripción de esa) puede ser equivocada.

El acercamiento alterno, la selección de modelo ofrece una alternativa en donde uno compara múltiples hipótesis y produce su inferencia de estos resultados. La selección de modelos es basado en la teoría de verosimilitud (likelihood). Las ventajas es que el investigador no esta limitado a evaluar un solo modelo donde el umbral es uno que es arbitrario (el modelo nulo con un valor de p \<0.05). Los modelos pueden ser comparado uno con el otro y organizado por su peso (**weight** o sea el apoyo estadístico), ofreciendo una medida relativa de apoyo contra las otras hipótesis. En caso que hay modelos que tienen el mismo apoyo uno puede seleccionar los mejores modelos y calcular un modelo promedio.

# Como funciona la selección de modelos?

El acercamiento filosófico de selección de modelos es evaluar múltiples hipótesis y la evidencia que apoya cada una. Por consecuencia el primer paso es articular un grupo de hipótesis razonables. El segundo paso es adaptar cada hipótesis a los datos observados con una función matemática. El tercer paso es seleccionar un método de selección de modelos y comparar los resultados.

## $R^2$

El acercamiento tradicional en la literatura es el $R^2$ o el coeficiente de determinación. Que es un acercamiento sencillo de selección de modelo. Donde mayor es el valor de $R^2$ mayor es el ajuste (Fit) del modelo. Este acercamiento no toma en cuenta la complejidad de los modelos y siempre selecciona modelos más complejos. La razón es que selecciona modelos más complejos es que a añadir más variables a un modelo (por ejemplo regresión lineal múltiples), añadir otra variable explica en parte la variación aunque esta nueva variable explica muy poco, pero ya que explica un componente aumenta el $R^2$ . Por consecuencia no toma en cuenta el concepto de parsimonia, donde deberíamos seleccionar el modelos más sencillo tomando encenta el ajuste y la complejidad del modelo. Un método de selección de modelos debería tomar en cuenta la complejidad de los modelos y penalizar por modelos excesivamente complejos.

### Ejemplo de Construcción de selección por $R^2$

Seleccionamos los datos de modulo regresión multiples (TF5). Sobre el **Biomass Index** y comoo esta corelacionado con la edad, el nivel de colesterol y glucosa en la sangre.

```{r,modelo}

library(readxl)
reg.multiple <- read_excel("Data/mod_empiricos.xlsx", sheet = "bmi")
# para ver las primeras seis filas de datos
head(reg.multiple)


```

#### Installar los siguientes paquetes si no lo tienen instalado 
```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load("MuMIn", "formattable")

```


Se construye un modelo con todos los parametros. Vemos que el $R^2= 0.0785$.

```{r, modelo basico}
library(car)
library(sjPlot)
model <- lm(BMI ~ Age + Cholesterol + Glucose, data = reg.multiple)
tab_model(model)

```

------------------------------------------------------------------------

Ahora evaluamos todas las posibles combinaciones de modelos. Considera la columna de $R^2$ y los valores para cada modelo. Los que se observa es que el modelo con las tres variables tiene un $R^2$ más alto seguido por los modelos con dos variables. La pregunta principal, un modelo con tres variables con un $R^2=.127$ es significativamente mejor que un modelo de 2 variables con un $R^2=.1185$. Usando el método tradicional no hay mecanismos para seleccionar y evaluar cual de los modelos es el mejor tomando en cuenta la complejidad del modelo. NOTA: ***NA*** en la tabla quiere decir que esta variable no esta incluida en el modelo.

```{r, dredge, message=FALSE}
library(MASS)
library(MuMIn)

options(na.action = "na.fail")
model2=lm(BMI ~ Age + Cholesterol + Glucose, data = reg.multiple) # El Modelo más completo
#model2
#summary(model2)

ALL_models=dredge(model2, extra="R^2") # Evaluación de todas las compbinaciones/alternativas (modelos)
```

```{r, tabla R2, message=FALSE}
library(formattable)

formattable(ALL_models, digits=3, format="html")

```

# Pruebas de modelos nulos

La prueba de verosimilitud (PV) es el método más utilizado de las pruebas de hipótesis nula. La PV compara pares de modelos, cuando el la verosimilitud de un modelos más complejo es significativamente más grande que el modelo sencillo, el modelo complejo es aceptado y vise versa. Tipicamente se usa como indice el ji cuadrado $\chi^2$. En este caso al contrario del $R^2$, la selección de un modelo más complejo cuando tiene un PV más grande tiene beneficio aunque sea más complejo el modelo. La desventaja es que prueba no es independiente por consecuencia infla el error de tipo I o sea el $\alpha$. Otro punto importante es que la complejidad se añade de forma sucesiva a los modelos, por consecuencia

------------------------------------------------------------------------

Tabla de métodos comunes para selección de modelos

En la siguiente tabla tenemos mencionado cinco métodos de selección de modelos. El primero que fue desarrollado fue el de **Akaike Information criterion** ( ref), por consecuencia es el más utilizado por ser el más conocido. Pero hay múltiples otros que no están mencionado aquí como el Bayes Factor y el Mallow's $C_p$. Puede encontrar más información en el siguiente enlace.

<https://en.wikipedia.org/wiki/Model_selection#Criteria>

| Selección de modelos | Formula                                                                                               | Criterio de selección                                                    |
|----------------------|----------------------------------------|------------------------------------------------------------|
| $Verosimilitud$      | $\\-2\left\{\ln\left[L(\theta_p\ |y\right)]+\ln\left[L(\theta_{p+q}\ | y\right)]\right\}$ | Ajuste |                                                            |
| $AIC$                | $\\AIC=-2\ln\left[L(\theta_p\ |y\right)]+2p$ | Ajuste y complejidad                                                |
| $AIC_c$              | $\\AIC_c=-2\ln\left[L(\theta_p\ |y\right)]+2p\left(\frac{n}{n-p-1}\right)$ | Ajuste y complejidad, con corrección para tamaño de muestra pequeña |
| $Schwartz$           | $\\SC=-2\ln\left[L(\theta_p\ |y\right)]+p\cdot\ln\left(n\right)$ | Ajuste y complejidad, tamaño de muestra                             |
| $R_{adj}^2$          | $R_{adj}^2=1-\frac{RSS/n-p-1}{\frac{\sum_{n=i}^n\left(y_i-\overline{y}\right)^2}{n-1}}$             | Ajuste                                                     |

\*\* Las ideas y conceptos mencionado siguen en parte Johnson and Omland 2004.

------------------------------------------------------------------------

## Cual método es más apropriado?

Métodos que maximizan la verosimilitud solamente tiene una limitación respecto a la parsimonia del modelo. En múltiples áreas de estudio se esta moviendo a métodos que no incluye solamente la verosimilitud pero la complejidad de los modelos. En general el método que más se usa es el AIC en parte porque esta fundado en el Criterio de Información de **Kullback-Leibler**, pero hay otros que prefieren por ejemplo el criterio de información de Schwarz debido que este selecciona modelos más parsimonia. Nota que en este último toma en cuenta no solamente el ajuste, la complejidad pero el tamaño de muestra. El Schwarz es conocido también como el **Bayesian Information Criterion**, BIC, aunque no tiene nada de Bayesiano en el método de análisis. Para aclarar, el BIC no usa información previa para hacer los cálculos, y se puede usar tanto con análisis tradicional como Bayesiano.

***

## Estimación de parámetros y múltiples modelos

En muchos estudios el objetivo principal es estimar los parámetros para poder inferir algún proceso biológico o comportamiento humano. Por ejemplo, ¿*cual es la dosis apropiada de algún antibiótico para reducir el crecimiento de bacteria y el tiempo del tratamiento?*. Cuando hay un buen apoyo para un modelo especifico, los estimados de los parámetros de la verosimilitud pueden ser utilizados. Pero de vez en cuando hay apoyo para múltiples modelos, en otra palabra hay apoyo iguales para múltiples modelos, lo que resulta un problema seleccionar un modelo que sea **mejor** que otro. En este caso si hay más de un modelo se utiliza el promedio de los modelos. Los estimados de los parámetros de un modelo promedio son robustos en dos aspectos, 1) reduce el sesgo de seleccionar modelos y 2) toma en cuenta la incertidumbre en los modelos.

***

## Selección de Modelos entre muchos

¿Como seleccionar el **mejor** modelo entre muchos? Si por ejemplo usamos el indice de Akaike (AIC), cada modelo fue ajustado a los datos y el indice de $AIC_i$ fue calculado, la diferencia entre los valores de AIC, $\Delta_i$ y el mejor modelo, $AIC_{\min}$ es calculado. El **mejor** modelo en el conjunto evaluado es el modelo con un AIC más pequeño, $AIC_{\min}$.

$\Delta _i=AIC_i-AIC_{\min }$

La verosimilitud (likelihood) de un modelo, $g_i$, dado los datos, $y$, es calculado de la siguiente forma.

$L(g_i\ |y)=\exp\left(\frac{-1}{2\cdot\Delta_i}\right)$,

Comparar pares de modelos, particularmente el **mejor** modelo y los otros, tiene un índice que se llama **evidence ratio, ER** y traducido aquí como **la razón de evidencia.**

$ER=\frac{L(g_{mejor}\ |y)}{L(g_i\ |y)}$,

Los valores de verosimilitud se pueden normalizar para que la suma de los modelos sea 1.

$W_i=\frac{\exp\left(-\frac{1}{2\cdot\Delta_i}\right)}{\sum_{j=i}^R\exp\left(-\frac{1}{2\cdot\Delta j}\right)}$,

Este valor conocido como el **weight de Akaike**, o el peso de los modelos puede ser interpretado como una probabilidad que el modelo $i$ es el mejor modelo de los modelos evaluados.

***

## Model Averaging:

Cuando no hay un modelo único apoyado por los datos, no hay apoyo sustancial para un modelo único. Modelos que no tienen mucho apoyo pudiesen incluir ciertas condiciones, por ejemplo si se usa el peso de los modelos y este no es muy grande $w_{best}<0.09$ o si la diferencia entre los modelos tienen un $\Delta AIC$ de menor 2. En este caso se necesita calcular los promedios ponderado de los parámetros, $\tilde{\theta}$.

$\tilde{\theta}=\sum_{n=1}^Rw_i\tilde{\theta}_i$

donde el $\tilde{\theta}_i$ es el estimado de $\tilde{\theta}$ del modelo $i_{th}$ de los \*mejores\* modelos. De esta forma los parámetros del modelo están ponderado por su apoyo $w_{best}$. En adición se puede calcular la varianza de los parámetros con la siguiente formula.

$\hat{var}(\tilde{\theta})=\sum_{n=1}^Rw_i[\hat{var}\left(\tilde{\theta}\ |g_i\right)+\ (\tilde{\theta}-\hat{\theta})^2 ]$

donde $\hat{var}\left(\tilde{\theta}\ |g_i\right)$ es el estimado de la varianza de $\theta$ del modelo $i_{th}$. El estimador de varianza puede ser utilizado para evaluar la precisión de los estimados del conjunto de modelos considerados. Esto permite generar intervalos de confianza de los parámetros para tomar en consideración la incertidumbre de los modelos de selección.

`r colorize("hacer una lista de las condiciones para selección de modelos con su referencias", "red")`

***
## Inferencias de los modelos de selección

La selección de modelos es una herramienta para inferir procesos no observados basado en datos que demuestra un patrón. Datos que claramente apoya una hipótesis entre muchas evaluadas puede inferir procesos que pudiese haber generado los datos observados.


***
## Paso a Paso

### Primer paso: Construcción de los modelos

El primer paso es construir los modelos que queremos comparar, para facilitar el ejemplo usamos un modelos complejo y comparamos todas las diferentes alternativas. NOTA: que este no es necesariamente el mejor acercamiento, ya que uno debería tener a priori una serie de modelos candidatos.

Los datos son para evaluar el cambio de nivel de ansiedad y depresión en la población de Puerto Rico luego del Huracán María. Los datos presentados son parciales e incluyen solamente uno de los indices del nivel de depresión después de 5 o más semanas luego del Huracán (Tremblay et al. sin publicar). La hipótesis es que algunos factores sociales y/o económicos influencian el nivel de depresión en la gente.

```{r}
library(readr)
DFMaria_student <- read_csv("Data/DFMaria_student.csv")
```

Metadata:

-   Sexo, 1= Mujer; 2 = Hombres

-   Escolaridad, mayor el número más escolaridad tiene

-   Num\_Pers\_Hogar, Cantidad de persona que comparte el hogar familiar

-   Sala\_Mens\_Antes, El salario mensual antes del Huracán María, mayor el número mayor el salario

-   Sala\_Mens\_Desp, El salario mensual después del Huracán María, mayor el número mayor el salario

-   Index\_Dep5T, Un indice de depresión 5 o más semanas después del Huracán.

### Limpiar los datos

El primer paso es remover todos los participantes donde le falta algún información.

```{r}
DFMaria_student=na.omit(DFMaria_student)

```

### Construcción del modelo

```{r}
head(DFMaria_student)

Huracan_Dep=lm(Index_Dep5T~factor(Sexo)+Escolaridad+Num_Pers_Hogar+
                Sala_Mens_Antes+ Sala_Mens_Desp, data=DFMaria_student)

```

#### Observar el resultado del modelo lineal completo.

```{r}
summary(Huracan_Dep)

```

#### Evaluar todas los modelos. 
Se ordena los modelos comenzando con el mejor, y si resta el valor de AICc con el mejor modelo, menor es el AICc mejor es el modelo. Los modelos que se estarán aceptando son aquellos que tienen un delta de menos <-2.00 (comparando con el mejor modelo), $\Delta_i=AIC_i-AIC_{\min}$.  En otra palabra los modelos que difieren más de un AICc de 2.0 no se aceptan como buenos modelos. En este caso los seis primeros modelos son igual de bueno, y no hay evidencia que uno es mejor que el otro. Cuando hay más de un modelo se tiene que calcular los promedios ponderados de los parámetros $\beta_i$ .  

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(huxtable)


Model_Selection<-tribble(
  ~AIC_BIC_delta, ~Interpretación, 
  "0-2", "Poca evidencia que los modelos son diferentes", 
  "2-4", "Evidencia que los modelos son diferentes", 
  ">4", "Mucha evidencia que los modelos son diferentes",
)

huxtable(Model_Selection) %>% 
  set_bold(1, 1:2) %>% 
  set_text_color(1, 1:2, "red") %>% 
  set_bottom_border(row = 1, col = everywhere, value = 1) %>%
set_bottom_border(row = 4, col = everywhere, value = 1) %>%
set_caption("Criterios de Selección de Modelos")
```

Con la función **dredge** del paquete **MuMIn** podemos evaluar todas las combinaciones de modelos

```{r}
library(MASS)
library(MuMIn)

options(na.action = "na.fail")
Huracan_Dep=lm(Index_Dep5T~Escolaridad+Num_Pers_Hogar+
                Sala_Mens_Antes+ Sala_Mens_Desp, data=DFMaria_student) # El Modelo más completo
#model2
#summary(model2)

ALL_models=dredge(Huracan_Dep, rank="AICc", extra = c(BIC, "R^2")) # Evaluación de todas las combinaciones/alternativas (modelos)

library(formattable)

formattable(ALL_models, digits=4, format="html")

```

***
#### Calcular el coeficientes ponderados

```{r, message=FALSE, warning=FALSE}
allmodels=format(round(ALL_models, digits=3), scientific=FALSE)
library(data.table)
write.csv(allmodels, "allmodelsdata.csv")

```

***
### Seleccionar los modelos 

Seleccionar los modelos que tienen un delta AICc menor de 2 y calcular el promedio de los coeficientes de los modelos. Nota el conjunto de coeficientes debajo **subset**, esto son los coeficientes de las variables del conjunto de modelos que tienen un delta AICc menor de 2. 
```{r}

# calculate the model average 
model.avg(ALL_models, subset = delta < 2)

# get the subset of the best models into a data frame
 bestModels <- get.models(ALL_models, subset=delta<2)
    bestModels <- model.avg(bestModels)
    
    
 bestModels=summary(bestModels)
# bestModels$coefficients
 
 
```

*** 

### Extraer los parametros del modelo, usando las sigueientes funciones
```{r}
bestModelsResults=data.frame(bestModels$coefmat.subset)
 names(bestModelsResults)
 bestModelsResults
BMR=setNames(cbind(rownames(bestModelsResults), bestModelsResults, row.names = NULL), 
         c("Variables", "Coef_Estimate", "Std_Error", "AdjustedSE", "z_value", "p_value"))
BMR=as.data.frame(BMR)
#BMR
is.data.frame(BMR)

```

***

### Intervalos de Confianza 
Los Intervalos de confianza de los parámetros se extraje del modelo usando la siguiente funciones. 
```{r}
library(ggplot2)
library(coefplot)

# Calcular el intervalo de confianza de 2.5% y de 97.5% 

BMR$LowerBound=BMR$Coef_Estimate-BMR$AdjustedSE*2
BMR$UpperBound=BMR$Coef_Estimate+BMR$AdjustedSE*2
BMR
names(BMR)

```
***

### Visualizar los coeficientes con los intervalos de confianza
```{r}
ggplot(BMR, mapping=aes(x=Variables, y = Coef_Estimate,
       ymin = LowerBound, ymax = UpperBound, colour=Variables))+
  geom_point()+
  geom_hline(yintercept=0.0, colour="blue")+
  coord_flip()+
  geom_pointrange()+
  xlab("Variables")+
  ylab("Mean Coefficients: Indice de depresión")+
  rlt_theme+
  theme(legend.position="none")

ggsave("Indice_depresión.png")

```

Referencias:

Johnson, J. B., K. S. Omland. 2004. Model selection in ecology and evolution. TRENDS in Ecology and Evolution. 19:101-108. doi: 10.1016/j.tree.2003.10.013.

------------------------------------------------------------------------

```{r, echo=FALSE, fig.show = "hold", out.width = "100%", fig.align = "default"}
knitr::include_graphics(c("Graficos/UPR_logos.png"))
```

***
> “Activities reported in this website was supported by the National Institute of General Medical Sciences of the National Institutes of Health under Award Number R25GM121270. The content is solely the
responsibility of the authors and does not necessarily represent the official views of the National
Institutes of Health.”