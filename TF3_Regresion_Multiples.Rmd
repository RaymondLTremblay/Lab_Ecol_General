---
title: "Regresion_Multiple"
output:
  html_document:
    css: tutorial.css
    fig_caption: yes
    highlight: pygments
   # theme: simplex
    toc: yes
    toc_float: yes
    
---
<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #437d66;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, fig.show = "hold", out.width = "50%", fig.align = "default"}
knitr::include_graphics(c("Graficos/Analitica.png", "Graficos/UPR_IPERT_logo.png"))
```


***
# Regresión Múltiple

#### Lista de paquetes a instalar


```{r packages}

if (!require("pacman")) install.packages("pacman")
pacman::p_load(car, EnvStats,tidyverse, coefplot, leaps,
              rmarkdown, ggplot2, MASS, multcomp, effects, 
              gplots,agricolae, caret )


# Activar los paquetes
library(ggversa)
library(knitr)
library(tidyverse)
library(coefplot)
library(leaps)
library(EnvStats)
library(MASS)
library(car) # Companion to applied regression
library(ggplot2)  # Data Visualization
library(effects)
library(gplots) 

```


***

#### Lista de archivos a usar
Los siguientes archivos deben estar disponible en su directorio de trabajo de R:

* [mod_empiricos.xlsx](https://drive.google.com/open?id=1kKD0vOoQCBjgoecqvdcdwFiJuiEyaz-U)  
* [strain_sex_chol.xlsx](https://drive.google.com/open?id=1ATmAlI0I6K9m2GG0uukhH_72zQkTJQps)
* [Titanic.xlsx](https://drive.google.com/open?id=1scOVpqtb6c9EbM90m0pZm3zg98Khde_o)


***

### Objetivos Generales

* Conocer y aplicar los métodos disponibles en R para obtener los parámetros de **modelos de regresión múltiple**, evaluar su idoneidad para describir los datos, e interpretar su significado.


***

# Regresión Lineal Múltiple
Cuando tenemos más de una variable predictora ("independiente"), la regresión lineal simple es una **regresión múltiple**. El objetivo es simultaneamente evaluar múltiples variables independientes. La idea es de determinar cual impacto cada variable independiente tiene sobre la variable dependiente considerando que hay otras variables independientes. 

La $\alpha$ es el intercepto, las $B_i$ son los coeficientes para cada variable independiente y el $\epsilon$ es el error.  

$$Y=\alpha+\beta_1X_{1}+\beta_2X_{2}+...+\beta_nX_{n}+\epsilon$$



#### Ejemplo 1 y Datos

Usaremos como ejemplo los datos **bmi** en el archivo **mod_empiricos.xlsx**.  Estos son datos de individuos adultos entre 21 y 79 años, con las siguientes variables: **BMI**, un índice de masa corporal ($kg/m^{2}$); **Age**, edad (*años*); **Cholesterol**, niveles de colesterol en sangre, ($mg/dL$); **Glucose**, niveles de glucosa en la sangre, ($mg/dL$).

Para leer los datos emplearemos el siguiente código (obtenido del menú **Import Dataset/From Excel**): Nota que en este archivo de Excel hay más de una hoja de calculo, hay que indicar cual es la hoja que uno quiere importar usando **sheet="..."**. 

```{r}
library(readxl)
reg.multiple <- read_excel("Data/mod_empiricos.xlsx", sheet = "bmi")
# para ver las primeras seis filas de datos
head(reg.multiple)
```

***

### Pruebas de supuestos para regresión múltiples

Cuando se utiliza regresión múltiple usando el método de mínimos cuadrados (**OLS**, ordinary least square), se asume que que cumple con los siguientes supuestos.  

* **Normalidad**: 
    + Gráficamente: 
      + Q-Q plot; 
    + Estadísticamente: 
      + prueba de Shapiro & Wilk (**shapito.test**)
      + Anderson-Darling (en el paquete **nortest** y la función **ad_test**). 
* **Independencia**: los valores de $Y$ son independientes entre si, lo asumimos (no hay autocorrelación).
* **Linealidad**: relación lineal entre variable dependiente y cada una de las independientes; gráficas individuales.  Puede arreglarse con transformación; prueba de Box-Tidwell.
* **Homocedasticidad**: varianza de la variable dependiente (residuales) no varía con los valores de las variables independientes; gráfica de los residuales.
* **Multicolinealidad**: las variables independientes no deben estar correlacionadas entre si.


***
#### Normalidad

* Evaluación gráfica de normalidad de la variable dependiente. Se observa que la mayoría de los datos son muy cerca de la linea, por consecuencia  se puede asumir que la variable dependiente tiene una distribución normal. 
```{r qqplot, message=FALSE}

ggplot(reg.multiple, aes(sample=BMI))+
  geom_qq()+
  geom_qq_line(colour="red")
```

*** 

### Pruebas de Normalidad


* Prueba estadística de normalidad **Shapiro & Wilk** ($H_0:distribución\enspace normal$) . En este caso tanto la prueba de Shapiro_Wilk (p > 0.07) como la **Anderson-Darling** (p > 0.12) tienen el mismo resultado, que uno acepta la hipótesis nula, que no hay evidencia que los datos no provienen de una distribución normal.


```{r shapiroW}
shapiro.test(reg.multiple$BMI)

library(nortest)
ad.test(reg.multiple$BMI)
```


***
#### Prueba de Autocorrelación

La prueba de **Durbin-Watson** nos permite evaluar si ocurre autocorrelación entre los residuales de la variable dependiente.  Por ejemplo, una variable que depende del tiempo, presenta autocorrelación.  La $H_0$ es que la autocorrelación en los residuales del modelo es 0.

* Prueba de **Durbin-Watson** para autocorrelación. 

Pasos:

  + Construir el modelo
  + Aplicar la prueba de autocorrelación sobre el modelo

La prueba de Durbin-Watson tiene un valor de p > 0.05, por consecuencia no hay evidencia que hay autocorrelación en los datos.  

```{r message=FALSE}
library(car)
model <- lm(BMI ~ Age + Cholesterol + Glucose, data = reg.multiple)
dbt <- durbinWatsonTest(model, simulate = TRUE)
dbt
```

***

#### Prueba de linealidad del modelo

* Prueba de Box Tidwell para linealidad. Esta prueba determina si el modelo sigue un patrón lineal. Los estimados de máxima verosimilitud de los parámetros de transformación se calculan mediante el método de Box y Tidwell (1962), que suele ser más eficaz que utilizar una rutina general de cuadrados mínimos no lineales (non linear least squares) para este problema. La hipótesis nula es que el patrón de relación entre las variables dependientes y indepedientes sean lineal. 

Se observa que el valor de p >0.05 para cada variable sugiriendo que no hay evidencia que la relación sea diferente a una regresión lineal. 
```{r}
library(car)
boxTidwell(BMI ~ Age + Cholesterol + Glucose, data = reg.multiple)
```

***

#### Prueba de multicolinealidad

* Matriz de correlación con la correlaciones de Pearson (asume distribución normal) y gráficas de puntos con líneas de regresión. En este gráfico se observa que el patrón entre cada una de las variables independientes. Con esta se determina si hay alta colinearidad entre las variables.  

La multicolinealidad se refiere a una situación en la que dos o más variables explicativas en un modelo de regresión múltiple están altamente relacionadas linealmente. Tenemos multicolinealidad perfecta si, por ejemplo, como en la ecuación anterior, la correlación entre dos variables independientes es igual a 1 o −1

Para dar un ejemplo sencillo si hubiesen dos variables con una correlación bien alta entre ambas, las dos variables incluyen un componente de variación compartido. Añadir las dos variables a un modelo de regresión múltiple no seria correcto, porque las dos variables explican la misma variación.  En este caso lo que se observa es que BMI y el colesterol tiene una correlación de 28%.  

```{r}
library(car)
#matriz de correlación
signif(cor(reg.multiple),3) 
```

***

### Gráfica de regresiones en parejas, con línea de regresión

```{r}
scatterplotMatrix(reg.multiple, ~ Age + Cholesterol + Glucose,
                  smooth = list(lty = 2), id = TRUE,
                  regLine = list(lty = 1, col = "red"),
                  col = "blue")
```
***

### Prueba de Glauber para multicolinealidad


El paquete "mctest" en R proporciona la prueba de Farrar-Glauber y otras pruebas relevantes para la multicolinealidad. Hay dos funciones a saber. "Omcdiag" e "imcdiag" en el paquete "mctest" en R que proporcionarán la verificación de diagnóstico general e individual para la multicolinealidad, respectivamente.



```{r}
library(mctest) # paquete para evaluar multicolinealidad
head(reg.multiple)
model=lm(BMI ~ Age + Cholesterol + Glucose, data = reg.multiple)

omcdiag(model)
```

Determinar cual de las variables son correlacionados usar la función **imcdiag**.  Encontrarán múltiples indices.  Vemos el indice de VIF. Si ese valor es mayor de 10, se dice hay un alto nivel de correlación con esta variable.  

Para detalles sobre los cálculos vea el siguiente enlace.

<https://datascienceplus.com/multicollinearity-in-r/>

```{r}
imcdiag(model)
```





***

### Modelos de regresión lineal múltiple

A continuación se calculan los parámetros de diversos modelos de regresión, y se incluye una prueba de homocedasticidad (homogeneidad de la varianza) para cada modelo.

#### Modelo completo (todas las variables)

```{r}
modRM <- lm(BMI ~ Age + Cholesterol + Glucose, 
              data = reg.multiple)
summary(modRM)
aic <- AIC(modRM)
sprintf("AIC = %.2f", aic)
# prueba de homocedasticidad (Non-constant Variance Score Test)
# Ho:la varianza es constante en el ámbito de la predicción de Y
ncvTest(modRM)  # para determinar si la varianza es constante. "Igualdad de varianza" 
spreadLevelPlot(modRM)
```


#### Modelo eliminando la variable Age, por ser la menos correlaciona con BMI, y tener la mayor correlación con Glucose

```{r}
modRM <- lm(BMI ~ Cholesterol + Glucose, 
              data = reg.multiple)
summary(modRM)
aic <- AIC(modRM)
sprintf("AIC = %.2f", aic)
# prueba de homocedasticidad
ncvTest(modRM)
spreadLevelPlot(modRM)
```

***

#### Modelo con interacción entre Age y Glucose
Para denotar interacción entre variables se usa el símbolo __( : )__   Para incluir las variables solas y su interacción se utiliza el símbolo __( * )__

```{r}
modRM <- lm(BMI ~ Cholesterol + Glucose:Age, 
              data = reg.multiple)
summary(modRM)
aic <- AIC(modRM)
sprintf("AIC = %.2f", aic)
# prueba de homocedasticidad
ncvTest(modRM)
spreadLevelPlot(modRM)
```

*** 



### Selección automática de modelo - método "stepwise"
Existen métodos para seleccionar automáticamente el mejor modelo, a base de estadísticas indicadoras, y que conlleva un procedimiento iterativo.  Uno de estos procedimientos es conocido como 'stepwise' (por pasos), y aunque no es el más aceptado en la actualidad, ha sido muy usado y es una buena manera de ilustrar el procedimiento.

En este procedimiento el proceso de selección se basa en seleccionar el modelo que minimiza el valor de la estadística **AIC** (Akaike Information Criterion), que indica el modelo con la menor pérdida de información y mayor simplicidad.  En el proceso se parte de un modelo nulo (no efecto de predictores) y hasta un modelo muy complejo, incluyendo interacciones.  Las variables se incluyen y se quitan, y cada vez se recalcula AIC, hasta obtener el modelo que mantiene el mínimo valor de AIC.

```{r}
#formulación de un modelo nulo y un modelo completo
modNulo <- lm(BMI ~ 1, data = reg.multiple)
modFull <- lm(BMI ~ Cholesterol*Glucose + Age*Cholesterol + Age*Glucose, 
              data = reg.multiple)
#procedimiento stepwise
bmistep <- step(modNulo,
                scope = list(lower=modNulo, upper=modFull,
                             direction="both"))

summary(bmistep)
```

***
### Comparación de modelos   

#### Gráfica de coeficientes
Una manera de comparar modelos visualmente (en realidad sus coeficientes) es usar el paquete __coefplot__, en conjunto con __ggplot2__, para crear una gráfica de los coeficientes estimados de cada variable (sola o de interacción), en cada modelo y detectar los que son diferentes de 0, y los modelos que los contienen.

```{r}
library(ggplot2)
library(coefplot)
#cálculo para todos los modelos
modbmi1 <- lm(BMI ~ Age + Cholesterol + Glucose, data=reg.multiple)
modbmi2 <- lm(BMI ~ Age*Glucose + Cholesterol*Glucose + Age*Cholesterol, data=reg.multiple)
modbmi3 <- lm(BMI ~ Cholesterol + Age:Glucose, data=reg.multiple)
modbmi4 <- lm(BMI ~ Cholesterol + Glucose, data=reg.multiple)
modbmi5 <- lm(BMI ~ Cholesterol, data = reg.multiple)
#comparando coeficientes de todos los modelos
multiplot(modbmi1, modbmi2, modbmi3, modbmi4, modbmi5, pointSize = 2, intercept=FALSE)
```


***

#### Selección de modelo usando R-cuadrado ajustado y Mallow's Cp para mejores modelos

La estadística $R^2$ es la cantidad de varianza en la respuesta (variable dependiente) producida por las variables predictoras, mediante el modelo, por lo tanto constituye una buena manera de medir la capacidad del modelo para "explicar" los datos.  Sin embargo, en un modelo de regresión múltiple, el $R^2$ aumenta al aumentar el número de predictores, lo cual conlleva a sobrestimar la "calidad" del modelo, con un número excesivo de variables.  Usando el $R^2$ ajustado se toma en cuenta el número de parámetros en el modelo, por lo tanto es una medida más realista del ajuste al modelo.

La estadística de Mallow, $C_p$, es otro indicador para seleccionar el mejor modelo en una regresión múltiple; funciona de manera similar al AIC, y sirve para evitar incluir parámetros en exceso.  La regla general es escoger el modelo con el número de parámetros, en el cual el valor de $C_p$ sea cercano (pero menor) al número de parámetros más 1.

```{r, fig.width=7, fig.height=7}
library(leaps)
modSS <- regsubsets(BMI ~ Age*Cholesterol + Cholesterol*Glucose + Age*Glucose, data = reg.multiple, nbest = 3, intercept = TRUE)
# gráfica para R^2 ajustado
plot(modSS, scale="adjr2")
# gráfica para Cp
plot(modSS, scale="Cp")
# otra forma de visualizar Cp
library(car)
## Mallow Cp
mallowCp <-
    subsets(modSS, statistic="cp", legend = FALSE, min.size = 1, main = "Mallow Cp")
abline(a = 1, b = 1, lty = 2)
```


***
### EJERCICIO 

1. Usar los datos [deathsmall_cities.xlsx](https://drive.google.com/open?id=1Q5rkNpatiTDg90ieVbwAw_fziIQNA4eJ) para buscar un modelo de regresión múltiple entre la variable dependiente _death1K_ (muertes anuales por cada 1000 habitantes) y las otras variables del archivo.

```{r}
library(readxl)
death_small_cities <- read_excel("Data/death_small_cities.xlsx", sheet=2)
dsc=death_small_cities
head(dsc)
```

## Normalidad

### Visualización
```{r qqplot2, message=FALSE, echo=FALSE}

ggplot(dsc, aes(sample=death1K))+
  geom_qq()+
  geom_qq_line(colour="red")
```

### Pruebas 
```{r shapiroW2, message=FALSE, echo=FALSE}
shapiro.test(dsc$death1K)

library(nortest)
ad.test(dsc$death1K)
```
### Prueb a de autocorelación de Durbin Watson

```{r, message=FALSE, echo=FALSE}
names(dsc)
library(car)
model1 <- lm(death1K ~ doctor100K+hosp100K+income1K+density, data = dsc)
dbt1 <- durbinWatsonTest(model1, simulate = TRUE)
dbt1
```



###  Multicolienarity

```{r multicol, message=FALSE, echo=FALSE}
signif(cor(dsc),3) 
```


### Visualizando colinearidad

```{r, message=FALSE, echo=FALSE}
# Alternativas para evaluar correlación entre variables
library(GGally)

ggcorr(dsc)  # GGally
# la siguiente función
ggpairs(dsc)
# la siguiente función
ggduo(dsc)
```

### El Modelo

```{r, message=FALSE, echo=FALSE}
mod1 <- lm(death1K ~ doctor100K+hosp100K+income1K+density, data = dsc)

summary(mod1)
aic <- AIC(mod1)
sprintf("AIC = %.2f", aic)
# prueba de homocedasticidad (Non-constant Variance Score Test)
# Ho:la varianza es constante en el ámbito de la predicción de Y
ncvTest(mod1)  # para determinar si la varianza es constante. "Igualdad de varianza" 
spreadLevelPlot(mod1)
```

### Step wise Regresión multiple

```{r}
#formulación de un modelo nulo y un modelo completo
modNulo1 <- lm(death1K ~ 1, data = dsc)
modFull1 <- lm(death1K ~ doctor100K+hosp100K+income1K+density, 
              data = dsc)
#procedimiento stepwise
bmistep2 <- step(modNulo1,
                scope = list(lower=modNulo1, upper=modFull1,
                             direction="both"))

summary(bmistep2)
```




```{r, echo=FALSE, fig.show = "hold", out.width = "100%", fig.align = "default"}
knitr::include_graphics(c("Graficos/UPR_logos.png"))
```

***

> “Activities reported in this website was supported by the National Institute of General Medical Sciences of the National Institutes of Health under Award Number R25GM121270. The content is solely the
responsibility of the authors and does not necessarily represent the official views of the National
Institutes of Health.”